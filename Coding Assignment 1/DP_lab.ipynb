{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stDkvxR1KTgV"
   },
   "source": [
    "# Reinforcement Learning - Dynamic Programming\n",
    "The goal of this assingment is to make you familiar with (gym) environments, MDP dynamics and basic concepts which are used in Reinforcement Learning. If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iRzRVyoaKTgZ"
   },
   "source": [
    "# This cell imports %%execwritefile command (executes cell and writes it into file).\n",
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x90x_38YKTgc"
   },
   "source": [
    "%%execwritefile dp_autograde.py\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O9Rsszo1KTge"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uzX2UssKTgf"
   },
   "source": [
    "## 1. Policy Evaluation (1 point)\n",
    "In this exercise we will evaluate a policy, e.g. find the value function of a policy. The problem we consider is the gridworld from Example 4.1 in the book. The environment is implemented as `GridworldEnv`, which is a subclass of the `Env` class from [OpenAI Gym](https://github.com/openai/gym). This means that we can interact with the environment. We can look at the documentation to see how we can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EHqrRH2-KTgg"
   },
   "source": [
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()\n",
    "# Lets see what this is\n",
    "?env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zE7EQVIxKTgi"
   },
   "source": [
    "# To have a quick look into the code\n",
    "??env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2ctdxp_KTgj"
   },
   "source": [
    "Now we want to evaluate a policy by using Dynamic Programming. For more information, see the [Intro to RL](https://drive.google.com/open?id=1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG) book, section 4.1. This algorithm requires knowledge of the problem dynamics in the form of the transition probabilities $p(s',r|s,a)$. In general these are not available, but for our gridworld we know the dynamics and these can be accessed as `env.P`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OaIsxXuhKTgj"
   },
   "source": [
    "# Take a moment to figure out what P represents.\n",
    "# Note that this is a deterministic environment.\n",
    "# What would a stochastic environment look like?\n",
    "env.P"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ld9yC66HKTgl"
   },
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_eval_v(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment.\n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "\n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a all 0 value function\n",
    "    V = np.zeros(env.nS)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            updated_v = 0\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    updated_v += (prob * (reward + discount_factor * V[next_state])) * policy[s][a]\n",
    "\n",
    "            delta = max(delta, abs(V[s] - updated_v))\n",
    "            V[s] = updated_v\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return np.array(V)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "coPaUWaMKTgm"
   },
   "source": [
    "# Let's run your code, does it make sense?\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval_v(random_policy, env)\n",
    "assert V.shape == (env.nS)\n",
    "V"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d-FhwvseKTgn"
   },
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84b4CHIqKTgo"
   },
   "source": [
    "---\n",
    "## 2. Policy Iteration (2 points)\n",
    "Using the policy evaluation algorithm we can implement policy iteration to find a good policy for this problem. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "juRiWkVhKTgo"
   },
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_iter_v(env, policy_eval_v=policy_eval_v, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Iteration Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "\n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_v: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (policy, V).\n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "\n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    # YOUR CODE HERE\n",
    "    while True:\n",
    "        V = policy_eval_v(policy, env, discount_factor)\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in range(env.nS):\n",
    "            action_value = np.zeros(env.nA)\n",
    "            old_action = policy[s]\n",
    "\n",
    "            for a in range(env.nA):\n",
    "                for (prob, next_state, reward, done) in env.P[s][a]:\n",
    "                    action_value[a] += prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            policy[s][:] = 0\n",
    "            policy[s][np.argmax(action_value)] = 1\n",
    "\n",
    "            if np.any(old_action != policy[s]):\n",
    "                policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return (policy, V)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m1a-zKgqKTgp"
   },
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_iter_v(env, policy_eval_v)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r13ChM5aKTgq"
   },
   "source": [
    "---\n",
    "## 3. Q-value Iteration (3 points)\n",
    "In this exercise you will implement the value iteration algorithm. However, because this algorithm is quite similar to the ones you implemented previously, we will spice things up a bit and use Q-values instead. Thus instead of using Bellman equations for V you will use Bellman equations for Q.   "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EvEP7UKrKTgq"
   },
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def value_iter_q(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Q-value Iteration Algorithm.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment.\n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all state-action pairs.\n",
    "        discount_factor: Gamma discount factor.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (policy, Q) of the optimal policy and the optimal Q-value function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start with a all 0 Q-value function\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    while True:\n",
    "        delta = 0 \n",
    "\n",
    "        for state in range(env.nS):\n",
    "            for action in range(env.nA):\n",
    "                q_current = Q[state, action]\n",
    "                q_value = 0\n",
    "\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    q_value += prob * (reward + discount_factor * np.max(Q[next_state]))\n",
    "\n",
    "                delta = max(delta, abs(q_current - q_value))\n",
    "                Q[state, action] = q_value\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    policy = np.zeros((env.nS, env.nA))\n",
    "    best_actions = np.argmax(Q, axis=1)  \n",
    "\n",
    "    for state in range(env.nS):\n",
    "        policy[state, best_actions[state]] = 1\n",
    "\n",
    "\n",
    "    return policy, Q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fqvpLAfHKTgr"
   },
   "source": [
    "# Let's see what it does\n",
    "policy, Q = value_iter_q(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Q Function:\")\n",
    "print(Q)\n",
    "print(\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ez4Pv8sdKTgs"
   },
   "source": [
    "# As you can see visualization of a\\the Q function is quite clumsy and is not that easy to check that values make sense.\n",
    "# However, you can easily create a V function from Q and policy to double check that the values are what you expect."
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
