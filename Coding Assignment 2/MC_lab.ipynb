{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca5ZSM-m4iCf"
   },
   "source": [
    "# Reinforcement Learning - Monte Carlo\n",
    "In this assignment you will use Monte Carlo methods to estimate a value function of a policy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A2jEG9bF4iCg"
   },
   "source": [
    "# This cell imports %%execwritefile command (executes cell and writes it into file).\n",
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9QvgoItY4iCi"
   },
   "source": [
    "%%execwritefile mc_autograde.py\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bBCnKCXB4iCi"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dy6POqV84iCi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ab207a9f93cf4d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhZTMA5C4iCj",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f0c1d608436b67b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For the Monte Carlo Prediction we will look at the Blackjack game (Example 5.1 from the book), for which the `BlackjackEnv` is implemented in `blackjack.py`. Note that compared to the gridworld, the state is no longer a single integer, which is why we use a dictionary to represent the value function instead of a numpy array. By using `defaultdict`, each state gets a default value of 0."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vVmYJep24iCj",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a342b69fcfdea5b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "from blackjack import BlackjackEnv\n",
    "env = BlackjackEnv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-RgLCl-4iCj"
   },
   "source": [
    "For the Monte Carlo algorithm, we no longer have transition probabilities and we need to *interact* with the environment. This means that we start an episode by using `env.reset` and send the environment actions via `env.step` to observe the reward and next observation (state)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "baK_NqHA4iCk",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85356add2643980e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# So let's have a look at what we can do in general with an environment...\n",
    "import gym\n",
    "?gym.Env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O_wfxa7r4iCk",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-251b7b17c5d08a24",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# We can also look at the documentation/implementation of a method\n",
    "?env.step"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V4vdDTJz4iCl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6decb2ab83c5bcec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "??BlackjackEnv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuA2aua14iCl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae161126d3cb1b7b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "A very simple policy for Blackjack is to *stick* if we have 20 or 21 points and *hit* otherwise. We want to know how good this policy is. This policy is *deterministic* and therefore a function that maps an observation to a single action. Technically, we can implement this as a dictionary or, a function or a class with a function, where we use the last option. Moreover, it is often useful (as you will see later) to implement a function that returns  the probability $\\pi(a|s)$ for the state action pair (the probability that this policy would perform certain action in given state). We group these two functions in a policy class. To get started, let's implement this simple policy for BlackJack."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9wYDrCkj4iCm",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9fdcb503df9cdb08",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "class SimpleBlackjackPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple BlackJack policy that sticks with less than 20 points and hits otherwise.\n",
    "    \"\"\"\n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains a probability\n",
    "        of perfoming action in given state for every corresponding state action pair.\n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        probs = []\n",
    "\n",
    "\n",
    "        for i in range(len(states)):\n",
    "            if states[i][0] >= 20:\n",
    "                if actions[i] == 0: #stick\n",
    "                    probs.append(1) #bigger then 20, thus stick is prob 1\n",
    "                else: #hit\n",
    "                    probs.append(0)\n",
    "            else:\n",
    "                if actions[i] == 0: #stick\n",
    "                    probs.append(0) #smaller 20, thus stick is prob 0\n",
    "                else: #hit\n",
    "                    probs.append(1)\n",
    "\n",
    "        # states = np.array([s[0] for s in states]) >= 20\n",
    "        # _probs = states ^ np.array(actions)\n",
    "\n",
    "        return np.array(probs)\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.\n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        if state[0] >= 20:\n",
    "            action = 0 #stick\n",
    "        else:\n",
    "            action = 1 #hit\n",
    "\n",
    "\n",
    "        return action #0 if state[0] >= 20 else 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fwF6TX234iCm",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-99f02e2d9b338a5b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Let's check if it makes sense\n",
    "env = BlackjackEnv()\n",
    "s = env.reset()\n",
    "policy = SimpleBlackjackPolicy()\n",
    "print(\"State: {}\\nSampled Action: {}\\nProbabilities [stick, hit]: {}\".format(s, policy.sample_action(s), policy.get_probs([s,s],[0,1])))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bkxxKSE4iCn"
   },
   "source": [
    "Since there are multiple algorithms which require data from single episode (or multiple episodes) it is useful to write a routine that will sample a single episode. This will save us some time later. Implement a *sample_episode* function which uses environment and policy to sample a single episode."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R3INxW8k4iCn"
   },
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function and policy's sample_action function as lists.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of lists (states, actions, rewards, dones). All lists should have same length.\n",
    "        Hint: Do not include the state after the termination in the list of states.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    state = env.reset() \n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        states.append(state)\n",
    "        action = policy.sample_action(state)\n",
    "        actions.append(action)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return states, actions, rewards, dones"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wIxTdaUF4iCn"
   },
   "source": [
    "# Let's sample some episodes\n",
    "env = BlackjackEnv()\n",
    "policy = SimpleBlackjackPolicy()\n",
    "for episode in range(3):\n",
    "    trajectory_data = sample_episode(env, policy)\n",
    "    print(\"Episode {}:\\nStates {}\\nActions {}\\nRewards {}\\nDones {}\\n\".format(episode,*trajectory_data))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZKa8RNI4iCo",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0184f4c719afb98c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the MC prediction algorithm (either first visit or every visit). Hint: you can use `for i in tqdm(range(num_episodes))` to show a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ih4n7gPw4iCo"
   },
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0, sampling_function=sample_episode):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "\n",
    "    Args:\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "    V = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        states, actions, rewards, dones = sampling_function(env, policy)\n",
    "\n",
    "        G = 0\n",
    "\n",
    "        visited_states = []\n",
    "\n",
    "        for j in reversed(range(len(states))):\n",
    "            G = discount_factor * G + rewards[j]\n",
    "            state = states[j]\n",
    "\n",
    "            # first-visit monte carlo algorithm\n",
    "            if state not in visited_states:\n",
    "                visited_states.append(state)\n",
    "                returns_count[state] += 1\n",
    "                V[state] += (G - V[state]) / returns_count[state]\n",
    "\n",
    "    return V"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NEVip3_Q4iCp"
   },
   "source": [
    "V_10k = mc_prediction(SimpleBlackjackPolicy(), env, num_episodes=10000)\n",
    "print(V_10k)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQvkOxW74iCp",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d32f907f180c088",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now make *4 plots* like Figure 5.1 in the book. You can either make 3D plots or heatmaps. Make sure that your results look similar to the results in the book. Give your plots appropriate titles, axis labels, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h4AU9h_34iCp",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbaf4d6a0e4c00fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "%%time\n",
    "# Let's run your code one time\n",
    "V_500k = mc_prediction(SimpleBlackjackPolicy(), env, num_episodes=500000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E6dkGHib4iCq",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ba046443478aa517",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_blackjack_value_function(V, title=\"Value Function\"):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    player_sum = np.arange(12, 22)\n",
    "    dealer_show = np.arange(1, 11)\n",
    "    usable_ace = [True, False]\n",
    "\n",
    "    #fig = plt.figure(figsize=(15, 10))\n",
    "    fig, axes = plt.subplots(nrows=2, figsize=(15, 10), subplot_kw={'projection': '3d'})\n",
    "\n",
    "    X, Y = np.meshgrid(dealer_show, player_sum)\n",
    "    Z_ace1 = np.apply_along_axis(lambda _: V.get((_[0], _[1], usable_ace[0]), 0), 2, np.dstack([Y, X]))\n",
    "    Z_ace2 = np.apply_along_axis(lambda _: V.get((_[0], _[1], usable_ace[1]), 0), 2, np.dstack([Y, X]))\n",
    "\n",
    "    axes[0].plot_surface(X, Y, Z_ace1)\n",
    "    axes[1].plot_surface(X, Y, Z_ace2)\n",
    "\n",
    "    for i in range(len(usable_ace)):\n",
    "        axes[i].set_title(f\"{title} - Usable Ace: {usable_ace[i]}\")\n",
    "        axes[i].set_xlabel(\"Dealer Showing\")\n",
    "        axes[i].set_ylabel(\"Player Sum\")\n",
    "        axes[i].set_zlabel(\"Value\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "plot_blackjack_value_function(V_10k, \"10,000 steps\")\n",
    "plot_blackjack_value_function(V_500k, \"500,000 steps\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giOIBQJS4iCq"
   },
   "source": [
    "## 2. Off-policy Monte Carlo prediction\n",
    "In real world, it is often beneficial to learn from the experience of others in addition to your own. For example, you can probably infer that running off the cliff with a car is a bad idea if you consider what \"return\" people who have tried it received.\n",
    "\n",
    "Similarly, we can benefit from the experience of other agents in reinforcement learning. In this exercise we will use off-policy monte carlo to estimate the value function of our target policy using the experience from a different behavior policy. Our target policy will be the simple policy as defined above (stick if we have *20* or *21* points) and we will use random policy as a behavior policy. As a first step, implement a random BlackJack policy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CFqYd7vO4iCq"
   },
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "class RandomBlackjackPolicy(object):\n",
    "    \"\"\"\n",
    "    A random BlackJack policy.\n",
    "    \"\"\"\n",
    "    def get_probs(self, states, actions):\n",
    "        \"\"\"\n",
    "        This method takes a list of states and a list of actions and returns a numpy array that contains\n",
    "        a probability of perfoming action in given state for every corresponding state action pair.\n",
    "\n",
    "        Args:\n",
    "            states: a list of states.\n",
    "            actions: a list of actions.\n",
    "\n",
    "        Returns:\n",
    "            Numpy array filled with probabilities (same length as states and actions)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return np.ones(len(states)) * 0.5\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.\n",
    "\n",
    "        Args:\n",
    "            state: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return np.random.choice([0, 1])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajBp3YmN4iCq"
   },
   "source": [
    "# Let's check if it makes sense\n",
    "env = BlackjackEnv()\n",
    "s = env.reset()\n",
    "policy = RandomBlackjackPolicy()\n",
    "print(\"State: {}\\nSampled Action: {}\\nProbabilities [stick, hit]: {}\".format(s, policy.sample_action(s), policy.get_probs([s,s],[0,1])))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLyc91K14iCr"
   },
   "source": [
    "Now implement the MC prediction algorithm with ordinary importance sampling\n",
    "\n",
    "Hint: you can use for i in tqdm(range(num_episodes)) to show a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w79vkO0-4iCr"
   },
   "source": [
    "%%execwritefile -a mc_autograde.py\n",
    "\n",
    "def mc_importance_sampling(behavior_policy, target_policy, env, num_episodes, discount_factor=1.0,\n",
    "                           sampling_function=sample_episode):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given target policy using behavior policy and ordinary importance sampling.\n",
    "\n",
    "    Args:\n",
    "        behavior_policy: A policy used to collect the data.\n",
    "        target_policy: A policy which value function we want to estimate.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        sampling_function: Function that generates data from one episode.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of current V and count of returns for each state\n",
    "    # to calculate an update.\n",
    "    V = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    get_sampling_ratio = lambda s, a: target_policy.get_probs([s], [a])[0] / behavior_policy.get_probs([s], [a])[0]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        states, actions, rewards, dones = sampling_function(env, behavior_policy)\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "\n",
    "        for state, action, reward in reversed(list(zip(states, actions, rewards))):\n",
    "            if get_sampling_ratio(state, action) == 0:\n",
    "                break\n",
    "\n",
    "            G = discount_factor * G + reward\n",
    "\n",
    "            returns_count[state] += 1\n",
    "            V[state] += W / returns_count[state] * (G - V[state])\n",
    "\n",
    "            W *= get_sampling_ratio(state, action)\n",
    "\n",
    "    return V"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2JNjVrCI4iCr"
   },
   "source": [
    "%%time\n",
    "# Let's run your code one time\n",
    "V_10k = mc_importance_sampling(RandomBlackjackPolicy(), SimpleBlackjackPolicy(), env, num_episodes=10000)\n",
    "print(V_10k)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RGDeN6Qu4iCr"
   },
   "source": [
    "V_500k = mc_importance_sampling(RandomBlackjackPolicy(), SimpleBlackjackPolicy(), env, num_episodes=500000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pfa8UBg4iCr"
   },
   "source": [
    "Let's plot the V function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uiJTybx34iCr"
   },
   "source": [
    "plot_blackjack_value_function(V_10k, \"10,000 steps\")\n",
    "plot_blackjack_value_function(V_500k, \"500,000 steps\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
